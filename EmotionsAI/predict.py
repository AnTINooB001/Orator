import torch
from torchvision import transforms
from PIL import Image
import argparse
import mediapipe

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
from model import create_model

def predict(image_path, model_path, class_names):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —ç–º–æ—Ü–∏—é –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.
    """
    # --- 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ç–æ–π –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    model = create_model(num_classes=len(class_names)).to(device)
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
    model.load_state_dict(torch.load(model_path, map_location=device))
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
    model.eval()

    # --- 2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ---
    # –í–∞–∂–Ω–æ: –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¢–ê–ö–ò–ú–ò –ñ–ï, –∫–∞–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    transform = transforms.Compose([
        transforms.Grayscale(num_output_channels=1),
        transforms.Resize((48, 48)), # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ä–∞–∑–º–µ—Ä—É 48x48
        transforms.ToTensor(),
        transforms.Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))
    ])

    # --- 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ---
    try:
        img = Image.open(image_path)
    except FileNotFoundError:
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {image_path}")
        return

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º "batch" –∏–∑–º–µ—Ä–µ–Ω–∏–µ
    img_tensor = transform(img).unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = model(img_tensor)
        # –ü—Ä–∏–º–µ–Ω—è–µ–º Softmax, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        top_prob, top_catid = torch.topk(probabilities, 1)

    prediction = class_names[top_catid]
    confidence = top_prob.item()

    print(f"ü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è: {prediction.upper()}")
    print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.')
    parser.add_argument('--image', type=str, required=True, help='–ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.')
    args = parser.parse_args()

    # –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ (—É–±–µ–¥–∏—Å—å, —á—Ç–æ –ø–æ—Ä—è–¥–æ–∫ –≤–µ—Ä–Ω—ã–π, –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
    # –¢—ã —É–±—Ä–∞–ª 'disgust', –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–≤–µ—Ä—å –ø–æ—Ä—è–¥–æ–∫
    classes = ['angry', 'fear', 'happy', 'neutral', 'sad', 'surprise']
    model_checkpoint = '../EmotionsAI/checkpoints/run_2025-09-24_19-22-25/best_model.pth'

    predict(args.image, model_checkpoint, classes)